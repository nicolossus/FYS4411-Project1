%================================================================
\section{Method}\label{sec:Method}
%================================================================

%----------------------------------------------------------------
\subsection{Sampling Algorithms}
%----------------------------------------------------------------

%----------------------------------------------------------------
\subsubsection{Random Walk Metropolis}
%----------------------------------------------------------------

%----------------------------------------------------------------
\subsubsection{Langevin Metropolis-Hastings}
%----------------------------------------------------------------

%----------------------------------------------------------------
\subsection{Project Method 1}\label{sec:project method}
%----------------------------------------------------------------

%----------------------------------------------------------------
\subsection*{Monte Carlo method for evaluating integrals.}
%----------------------------------------------------------------
The Monte Carlo (MC) method for evaluating integrals is a stochastic method that samples evaluations of the function (or a property of the function) uniformly over the domain of the integral, and returns the mean value of these. As we increase the number of evaluations, the law of large numbers tells us that the mean value will approach the expectation value. When the dimensionality of the problem becomes large, it is computationally very costly to evaluate the integral using numerical methods.

%----------------------------------------------------------------
\subsection*{Markov Chain Monte Carlo}
%----------------------------------------------------------------
We want to evaluate the integral 
\begin{equation}
    I = \int_{D\in\mathcal{R^{n, d}}}\hat{Q}P(\mathbf{x_1}, \dots, \mathbf{x_n})d\mathbf{x_1}\dots d\mathbf{x_n},
\end{equation}
where $\hat{Q}$ is an operator that acts on the $n\times d$-dimensional probability distribution $P(\mathbf{x_1}, \dots, \mathbf{x_n})$, where $\mathbf{x_i}$ is $d$-dimensional vectors for $1\leq i\leq n$, and $i\in\mathbb{N}$. 

%----------------------------------------------------------------
\subsection*{Variational Monte Carlo}
%----------------------------------------------------------------
Given a Hamiltonian $\hat{H}$ and a trial wave function $\Psi_T$, the variational principle states that the expectation value of $\langle \hat{H} \rangle$, defined through

%----------------------------------------------------------------
\subsection*{Blocking}
%----------------------------------------------------------------

The blocking method was first popularized by H. Flyvbjerg and H. G. Petersen [a]. The idea behind blocking is to provide an alternative method for estimating the variance $\text{Var}(\hat{\theta})$ for the estimator $\hat{\theta} = \overline{X}$. It can be compared to dependent bootstrapping but with a much lower computational complexity of $\mathcal{O}(n)$. This allows for speedups compared to other methods with a big-O of $\mathcal{O}(n^2)$ or $\mathcal{O}(n \log n)$ [b]. \\\\
Let's assume that we have a sample size of $n = 2^d$ for any $d \in \mathbb{N}$ and an n-tuple $\hat{X} = (X_1, X_2, \cdots, X_n)$ of stationary time series which we assume are asymptotically uncorrelated. 


%https://aip.scitation.org/doi/pdf/10.1063/1.457480
%https://www.duo.uio.no/bitstream/handle/10852/68360/PhysRevE.98.043304.pdf?sequence=2&isAllowed=y

%----------------------------------------------------------------
\subsection{Computational Strategies}
%----------------------------------------------------------------

In this section, we present an assortment of computational strategies we will use.

%----------------------------------------------------------------
\subsubsection{Log Densities}
%---------------------------------------------------------------- 

As mentioned in \autoref{sec:Theory_scaling_and_opt}, we perform computations with the trial wave function in the log domain. The reason for this is two-fold. Firstly, the form of expressions that need to be evaluated becomes easier. Secondly, the use of logarithmic densities in the MCMC algorithms avoid computational over- and underflows as the evaluation of the ratio between the densities in the common formulation of the algorithms is then computed as the difference of the log densities. % Might need a rewrite...
Exponentiation is performed only when necessary. 

%----------------------------------------------------------------
\subsubsection{Parallelization}
%---------------------------------------------------------------- 
Parallelizing a single Markov chain is not a straightforward task. However, MCMC sampling with multiple chains is so-called \textit{embarrassingly parallelizable}, which means there is little to no effort to divide the workload as the computations are independent. We will therefore implement procedures where we let multiple chains sample independently in parallel. Note that each chain still needs sufficient time to converge towards the stationary distribution, i.e., each will still need a sufficient number of Monte Carlo cycles to propagate in its workload. 